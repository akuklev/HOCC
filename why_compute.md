Type theories must be computational
===================================



# Lavwere algebraic theories

School algebra is about transforming expressions. We learn basic operations such as addition and multiplication and basic identities they satisfy such as associativity, commutativity and distributivity. We compose basic operations to form expressions (= derived operations), and learn how to transform them by composing laws. 

High school algebra only deals with operations on some kind of numbers, in propositional logic we deal with operations (conjunction, disjunction, negation) on propositions. Actually, one can study operations abstractly, i.e. without specifying what they are intended to act on. That's precisely what universal algebra does: it studies finitely-generated sets of operations satisfying finitely-generated sets of identities.

Classical universal algebra usually deals with the case when operations are generated by constants, unary, and binary operations, operands being of the same sort as values. It is possible to generalize to the case with multiple sorts, e.g. vectors and scalars.

To generalize from propositional logic to predicate logic, we need an infinite number of sorts, namely the sort 0 of propositions, the sort 1 of propositions with one variable (unary predicates) and so forth for propositions with any finite number of variables (`n`-ary predicates). Quantifiers `∀` and `∃` bind one variable and thus decrement the sort of their operand. To state it precisely, quantifiers are not just two operations but an infinite families of operations: for any natural numbers `n < m` there is a specific variant of that accept m-ary predicates binding their `n`th hole. Binary operations like the conjunction and disjunction also turn into an infinite family. For unary predicates `R(x)` and `Q(x)` each binary operation * has two variants: it can either join the variables of predicates `R(x) * Q(x)` or leave them distinct `R(x) and Q(y)`. In the general case of an n-ary and and m-ary predicate, there will be many ways to join variables, but it is not hard to derive them all algorithmically. Thus, the set of operations is not finitely-generated anymore, but effectively generated (can be generated by an algorithm). The same applies to the set of identities, since we finite number of identities cannot govern an infinite number of primitive operations.

The gadget to deal with predicate logic algebraically is known as effectively generated Lawvere algebraic theory: it has a subcountable set of sorts, and effectively generated sets of operations and identities.

# Algebraic theories with dependent sorts

We already discussed operations acting on numbers, propositions, and predicates. Let us now consider operations acting on even more complex entities: proofs and programs.

A proof calculus is a two-level system much like vector spaces having the sort of vectors and the sort of scalars. The first layer is the layer of propositions and predicates we have already described as a Lawvere theory. In addition there will be the layer of proofs. Similar to propositions, we'll have sorts of proofs with `n` variables for each `n`. But it is more complicated than that. The sort of proof has to include the proposition or predicate being proven! Otherwise we cannot state the most basic operator on proofs `p ▸ q` that composes a proof of `A` and a proof of `A implies B` into a proof of  `B`. Sorts of proofs with `n` variables have to be of the form `Prf(p)`, where `p`is an `n`-ary predicate. We need to have dependent sorts: a feat Lawvere theories cannot accomodate.

In Lawvere algebraic theories, sorts are mere labels governing which expressions can be plugged together. An expression can be plugged into a hole of another expression if the sorts of the expression and the hole are identical. Sorts are syntactical entities, so it's OK to talk about them being identical, after all syntactical entities can be faithfully numbered and “identical” simply means “the same number”.

When generalising to allow value-dependent sorts `S(x)` we run into a problem. Values are semantic entities, so they can be equal without being represented by identical expressions. How can we even talk about sorts being identical or not if they depend on values?

Fortunately, values `x` occurring as sort arguments `S(x)` rules are not just any values, but values given by expressions of the very same theory, so it might be possible to provide an algorithm that computes canonical forms `|x|` so that equal values are mapped to identical normal forms. The operation `|_|` is where the computational aspect comes into play, which didn't occur in case of Lawvere theories.

For many sorts of interest it is is not possible to algorithmically extract canonical forms from values. For instance this is impossible for `n`-ary predicates with `n > 0`: word problem for their sorts is semiundecidable in the theory of predicate logic. If two predicates are indeed equal, systematic application of all possible identities will eventually find a path from one to another, but if they are distinct, this process will simply proceed indefinitely; we cannot in general prove distinctness of two predicates due to halting problem, which precludes existence of canonical forms. In such cases we can assign non-canonical normal forms `|_|` to values and find or introduce a (doubly dependent) sort `Id(a, b)` inhabited by identification paths from `a` to `b`. In our case we luckily already have one: two predicates can be shown equivalent precisely if we have a proof of the sort `Prf((A implies B) and (B implies A))`. This type has to have property that it allows to plug expressions of the sort `Prf(A)` into holes of the sort `Prf(B)` if we have `p : Id(x, y)`, that is `p : Prf((A implies B) and (B implies A))`. In simple cases can have an algorithm that syntactically transforms expressions of the sort `Prf(A)` into expressions of the sort `Prf(B)`. This is unfortunatelly not alwasy possible. What turns out to be possible is an algorithm that syntactically transform the recipient expression with hole of the sort `Prf(A)` into an equivalent expression with the corresponding hole of the type `Prf(B)` owing to additional flexibility that this syntactic transform can also change the resulting sort of the expression into an equivalent but non-identical one.


Тут про то, что мы можем иметь внутри теории типы — термы особого сорта *, представляющие сорта, то есть в качестве сортов мы можем использовать их нормальные формы e : |t|. Дальше сказать что у типов вместо равенства эквивалентность ххх, и она бывает разная, но мы можем поднимать выражения сдыркой вдоль identification path, как поднимали в случае значений используя доказательство равенства. И эта операция называется identification path lifting, и область математики которая описывает path lifting properties называется абстрактной теорией гомотопий — вот где эта штука вступает.

```
Prototypes are type-theoretical effective representations of Reedy categories.

Prototype is an inductive type `T : *` defined simultaneously with two following inductive-recursive types:
  ReductionsForm[T] : *, .to : T, .precompose  : (x : ReductionsFrom[.to]) -> ReductionsFrom[T] with .to ≡ x.to
  ExtensionsTo[T] : *, .from : T, .postcompose : (x : ExtensionsTo[.from]) -> ExtensionsFrom[T] with .from ≡ x.from

The definitional equalities must be checked. It is only possible if `t.to` and `t.from` are structurally smaller
then `t`. It ensures that the arguments `x` of the functions precompose and postcompose come from a type that
has already been defined, its constructors are known and values of .to and .from on resulting values can be
explicitly computed. The involution T° on prototypes simply exchanges reductions and extensions.

Now assume `F[t : T]` is a type dependent on the prototype `t`.

Values of type `x : F[t : T]` come with actions of reductions `rx : F[r.to]`.
Functions defined on variables `x : F[t : T]` by pattern matching must match “higher constructors”
`e : ExtensonsTo[x]` yielding an path f(x) = f(e.from) or an extension ExtensionTo[f(x)] with .from ≡ f(e.from)

Given an `x : F[t : T]` for every `r : ReductionFrom[t]` we have 


and every `e : ExtensionTo[T]`


Once a prototype is defined, we define types dependent on it by peculiar large elimination f : (t : T) -> *. 
For known values


on constructors of T itself yielding another type, but also on constructors of Extensions

```

A proof calculus for classical logic capable of structural induction over its own language.
